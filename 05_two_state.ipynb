{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-state design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/home/pleung/projects/bistable_bundle/r4/helix_binders\n",
      "dig175\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "# python internal\n",
    "import collections\n",
    "import copy\n",
    "import gc\n",
    "from glob import glob\n",
    "import h5py\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import socket\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# conda/pip\n",
    "import dask\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# special packages on the DIGS\n",
    "import py3Dmol\n",
    "import pymol\n",
    "import pyrosetta\n",
    "\n",
    "# notebook magic\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(os.getcwd())\n",
    "print(socket.gethostname())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original approach for multistate design of helix binders\n",
    "`/home/flop/switch/5thround/DHRs/allo3/msd1/msd_scripts/msd_fnr.py` is nonpolar interface  \n",
    "`/home/flop/switch/5thround/DHRs/allo3/msd3_pol/msd_scripts/msd_fnr.py` allows polars, while  \n",
    "`/home/flop/switch/5thround/DHRs/allo3/msd4_pol/msd_scripts/msd_fnr.py` uses constraints to force in polars\n",
    "I will use the serialization build of PyRosetta to enable recording user defined info about the designs.  \n",
    "This enables downstream inline filtering and data analysis, as well as clustering by lineage.\n",
    "\n",
    "Fix disulfides and chain labels, and optionally trim state X if it has a trailing loop  \n",
    "TODO make sure it is ok to trim the way it is now  \n",
    "TODO make a function that trims the smaller pose of X,Y safely (align by DSSP?)  for now, bases largely covered by trimming C term of state X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make functions for multi-state design\n",
    "no TRP constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrosetta.distributed.packed_pose.core import PackedPose\n",
    "from typing import *\n",
    "\n",
    "\n",
    "def msd(packed_pose_in: PackedPose, **kwargs) -> PackedPose:\n",
    "    \"\"\"\n",
    "    Assumes middle split, allowing backwards selection\n",
    "    \"\"\"\n",
    "\n",
    "    import bz2\n",
    "    from copy import deepcopy\n",
    "    import pyrosetta\n",
    "    from pyrosetta.rosetta.core.pose import Pose\n",
    "    import pyrosetta.distributed.io as io\n",
    "    from pyrosetta.distributed.tasks.rosetta_scripts import (\n",
    "        SingleoutputRosettaScriptsTask,\n",
    "    )\n",
    "    from pyrosetta.rosetta.core.pack.task.operation import (\n",
    "        OperateOnResidueSubset,\n",
    "        PreventRepackingRLT,\n",
    "        RestrictAbsentCanonicalAASExceptNativeRLT,\n",
    "        RestrictToRepackingRLT,\n",
    "    )\n",
    "    from pyrosetta.rosetta.core.select import get_residues_from_subset\n",
    "    from pyrosetta.rosetta.core.select.residue_selector import (\n",
    "        AndResidueSelector,\n",
    "        ChainSelector,\n",
    "        FalseResidueSelector,\n",
    "        InterGroupInterfaceByVectorSelector,\n",
    "        LayerSelector,\n",
    "        NeighborhoodResidueSelector,\n",
    "        NotResidueSelector,\n",
    "        OrResidueSelector,\n",
    "        ResidueIndexSelector,\n",
    "        TrueResidueSelector,\n",
    "    )\n",
    "\n",
    "    def yeet_pose_xyz(pose, xyz=(1, 0, 0)):\n",
    "        \"\"\"\n",
    "        Given a pose and a cartesian 3D unit vector, translates the pose\n",
    "        according to 100 * the unit vector without applying a rotation:\n",
    "        @pleung @bcov @flop\n",
    "        Args:\n",
    "            pose (Pose): The pose to move.\n",
    "            xyz (tuple): The cartesian 3D unit vector to move the pose in.\n",
    "\n",
    "        Returns:\n",
    "            pose (Pose): The moved pose.\n",
    "        \"\"\"\n",
    "        from pyrosetta.rosetta.core.select.residue_selector import TrueResidueSelector\n",
    "        from pyrosetta.rosetta.protocols.toolbox.pose_manipulation import (\n",
    "            rigid_body_move,\n",
    "        )\n",
    "\n",
    "        assert len(xyz) == 3\n",
    "        pose = pose.clone()\n",
    "        entire = TrueResidueSelector()\n",
    "        subset = entire.apply(pose)\n",
    "        # get which direction in cartesian unit vectors (xyz) to yeet pose\n",
    "        unit = pyrosetta.rosetta.numeric.xyzVector_double_t(*xyz)\n",
    "        scaled_xyz = tuple([100 * x for x in xyz])\n",
    "        far_away = pyrosetta.rosetta.numeric.xyzVector_double_t(*scaled_xyz)\n",
    "        rigid_body_move(unit, 0, far_away, pose, subset)\n",
    "        return pose\n",
    "\n",
    "    def combined_pose_maker(poses=[]) -> Pose:\n",
    "        \"\"\"\n",
    "        Combine up to 6 poses in a list into one multichain pose\n",
    "        \"\"\"\n",
    "        if len(poses) == 0:\n",
    "            raise RuntimeError(\"Empty list of poses passed\")\n",
    "        else:\n",
    "            pass\n",
    "        # get the first pose\n",
    "        new_pose = poses.pop(0).clone()\n",
    "        # unit vectors\n",
    "        xyzs = [(1, 0, 0), (0, 1, 0), (0, 0, 1), (-1, 0, 0), (0, -1, 0), (0, 0, -1)]\n",
    "        # go through rest of poses and add them into the first one\n",
    "        for i, pose in enumerate(poses):\n",
    "            xyz = xyzs[i]\n",
    "            to_append = yeet_pose_xyz(pose.clone(), xyz)\n",
    "            new_pose.append_pose_by_jump(\n",
    "                to_append,\n",
    "                new_pose.num_jump() + 1,  # last jump\n",
    "            )\n",
    "        return new_pose\n",
    "\n",
    "    poses = []\n",
    "    # load state Y\n",
    "    if packed_pose_in == None:\n",
    "        file = kwargs[\"-s\"]\n",
    "        with open(file, \"rb\") as f:\n",
    "            packed_Y = io.pose_from_pdbstring(bz2.decompress(f.read()).decode())\n",
    "        scores_Y = pyrosetta.distributed.cluster.get_scores_dict(file)[\"scores\"]\n",
    "        pose = io.to_pose(packed_Y)\n",
    "        for key, value in scores_Y.items():\n",
    "            pyrosetta.rosetta.core.pose.setPoseExtraScore(pose, key, value)\n",
    "        poses.append(pose)\n",
    "    else:\n",
    "        raise RuntimeError(\"Need to supply an input for state Y\")\n",
    "    # load state X\n",
    "    if kwargs[\"-x\"] != None:\n",
    "        file = kwargs[\"-x\"]\n",
    "        with open(file, \"rb\") as f:\n",
    "            packed_X = io.pose_from_pdbstring(bz2.decompress(f.read()).decode())\n",
    "        scores_X = pyrosetta.distributed.cluster.get_scores_dict(file)[\"scores\"]\n",
    "        pose = io.to_pose(packed_X)\n",
    "        for key, value in scores_X.items():\n",
    "            pyrosetta.rosetta.core.pose.setPoseExtraScore(pose, key, value)\n",
    "        poses.insert(0, pose)\n",
    "    else:\n",
    "        raise RuntimeError(\"Need to supply an input for state X\")\n",
    "\n",
    "    state_X, state_Y = poses[0], poses[1]\n",
    "\n",
    "    # check to see if trimming state X to the same length as state Y is needed\n",
    "    if len(state_X.residues) > int(state_Y.chain_end(1)):\n",
    "        pose_holder = pyrosetta.rosetta.core.pose.Pose()\n",
    "        for i in range(1, int(state_Y.chain_end(1)) + 1):\n",
    "            pose_holder.append_residue_by_bond(state_X.residue(i))\n",
    "        state_X = pose_holder.clone()\n",
    "        sw = pyrosetta.rosetta.protocols.simple_moves.SwitchChainOrderMover()\n",
    "        sw.chain_order(\"1\")\n",
    "        sw.apply(state_X)\n",
    "    elif len(state_X.residues) < int(state_Y.chain_end(1)):  # this is unexpected\n",
    "        return\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # paste in helix from state Y into X after yeeting in a safe direction\n",
    "    chB_alone = state_Y.clone()\n",
    "    sw = pyrosetta.rosetta.protocols.simple_moves.SwitchChainOrderMover()\n",
    "    sw.chain_order(\"2\")\n",
    "    sw.apply(chB_alone)\n",
    "    chB_alone = yeet_pose_xyz(chB_alone, xyz=(-1, 0, 0))  # yeet in opposite direction\n",
    "    state_X.append_pose_by_jump(chB_alone, state_X.num_jump() + 1)\n",
    "    sw = pyrosetta.rosetta.protocols.simple_moves.SwitchChainOrderMover()\n",
    "    sw.chain_order(\"12\")\n",
    "    sw.apply(state_X)\n",
    "    for key, value in scores_X.items():\n",
    "        pyrosetta.rosetta.core.pose.setPoseExtraScore(state_X, key, value)\n",
    "\n",
    "    # make sure there isn't the same disulfide between the states for some reason\n",
    "    if state_X.scores[\"disulfide_at\"] == state_Y.scores[\"disulfide_at\"]:\n",
    "        return None\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if kwargs[\"ala_pen\"] == None:\n",
    "        ala_penalty = 1\n",
    "    else:\n",
    "        ala_penalty = kwargs[\"ala_pen\"]\n",
    "    if kwargs[\"np_pen\"] == None:\n",
    "        np_penalty = 3\n",
    "    else:\n",
    "        np_penalty = kwargs[\"np_pen\"]\n",
    "    og_np_penalty = deepcopy(np_penalty)\n",
    "    og_ala_penalty = deepcopy(ala_penalty)\n",
    "    scores = deepcopy(scores_Y)\n",
    "    new_loop_resis = scores[\"new_loop_resis\"]\n",
    "    parent_sequence = state_X.sequence()\n",
    "    # heavily penalize buried unsats, unset lk_ball since it isn't worth using\n",
    "    # setup res_type_constraints for FNR, setup aa_comp, setup SAP constraint\n",
    "    sfxn_obj = pyrosetta.rosetta.protocols.rosetta_scripts.XmlObjects.create_from_string(\n",
    "        \"\"\"\n",
    "        <SCOREFXNS>\n",
    "            <ScoreFunction name=\"sfxn\" weights=\"beta_nov16\" />\n",
    "            <ScoreFunction name=\"sfxn_design\" weights=\"beta_nov16\" >\n",
    "                <Set use_hb_env_dep=\"true\" />\n",
    "                <Reweight scoretype=\"approximate_buried_unsat_penalty\" weight=\"17\" />\n",
    "                <Set approximate_buried_unsat_penalty_burial_atomic_depth=\"3.5\" />\n",
    "                <Set approximate_buried_unsat_penalty_hbond_energy_threshold=\"-1.0\" />\n",
    "                <Set approximate_buried_unsat_penalty_natural_corrections1=\"true\" />\n",
    "                <Set approximate_buried_unsat_penalty_hbond_bonus_cross_chain=\"-7\" />\n",
    "                <Set approximate_buried_unsat_penalty_hbond_bonus_ser_to_helix_bb=\"1\"/>\n",
    "                <Reweight scoretype=\"lk_ball\" weight=\"0\" />\n",
    "                <Reweight scoretype=\"lk_ball_iso\" weight=\"0\" />\n",
    "                <Reweight scoretype=\"lk_ball_bridge\" weight=\"0\" />\n",
    "                <Reweight scoretype=\"lk_ball_bridge_uncpl\" weight=\"0\" />\n",
    "                <Reweight scoretype=\"res_type_constraint\" weight=\"2.0\" />\n",
    "                <Reweight scoretype=\"aa_composition\" weight=\"1.0\" />\n",
    "                <Reweight scoretype=\"sap_constraint\" weight=\"1.0\" />\n",
    "            </ScoreFunction>\n",
    "        </SCOREFXNS>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    sfxn = sfxn_obj.get_score_function(\"sfxn_design\")\n",
    "    sfxn_clean = sfxn_obj.get_score_function(\"sfxn\")\n",
    "    res = scores[\"total_length\"]\n",
    "    score_per_res_X, score_per_res_Y = (\n",
    "        sfxn_clean(state_X) / res,\n",
    "        sfxn_clean(state_Y) / res,\n",
    "    )\n",
    "\n",
    "    def msd_fnr(\n",
    "        despose,\n",
    "        refpose,\n",
    "        weight=0.0,\n",
    "        strict_layers=False,\n",
    "        neighbors=False,\n",
    "        design_sel=None,\n",
    "        design_helix=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform multi state design (MSD) using FavorNativeResidue (FNR)\n",
    "        \"\"\"\n",
    "        true_sel = TrueResidueSelector()\n",
    "        allres = get_residues_from_subset(true_sel.apply(despose))\n",
    "        diff = pyrosetta.rosetta.utility.vector1_unsigned_long()\n",
    "\n",
    "        sel_chA = ChainSelector(\"A\")\n",
    "        chA = get_residues_from_subset(sel_chA.apply(refpose))\n",
    "        sel_chB = ChainSelector(\"B\")\n",
    "        chB = get_residues_from_subset(sel_chB.apply(refpose))\n",
    "        intsel = InterGroupInterfaceByVectorSelector(sel_chA, sel_chB)\n",
    "        intB = AndResidueSelector(intsel, sel_chB)\n",
    "        hlx_int = get_residues_from_subset(intB.apply(despose))\n",
    "        hh_int = get_residues_from_subset(intsel.apply(despose))\n",
    "\n",
    "        #  for the case where a disulfide reuses the same residue in both states we want to break the bond\n",
    "        des_dslfs = [int(i) for i in despose.scores[\"disulfide_at\"].split(\",\")]\n",
    "        ref_dslfs = [int(i) for i in refpose.scores[\"disulfide_at\"].split(\",\")]\n",
    "        # check each position for seq disagreement # TODO see if this fixes sequence length disagreement at c_term\n",
    "        for i in allres:\n",
    "            if despose.sequence(i, i) == \"C\":  # maintain disulfides in despose\n",
    "                continue\n",
    "            elif (\n",
    "                refpose.sequence(i, i) == \"C\"\n",
    "            ):  # safely replace despose residue with CYS (not CYD)\n",
    "                mut = pyrosetta.rosetta.protocols.simple_moves.MutateResidue()\n",
    "                mut.set_target(i)\n",
    "                mut.set_res_name(pyrosetta.rosetta.core.chemical.AA(2))  # 2 is CYS\n",
    "                mut.apply(despose)\n",
    "            elif despose.sequence(i, i) != refpose.sequence(i, i):\n",
    "                diff.append(i)\n",
    "                despose.replace_residue(i, refpose.residue(i), 1)\n",
    "            else:\n",
    "                pass\n",
    "        # optionally allow helixes to be designed\n",
    "        if design_helix:\n",
    "            for i in hlx_int:\n",
    "                diff.append(i)\n",
    "        # for the case where a disulfide reuses the same residue in both states we want to break the bond in refpose on despose\n",
    "        # use set math to determine if there is reuse\n",
    "        lone_dslfs = set(des_dslfs + ref_dslfs) - set(des_dslfs)\n",
    "\n",
    "        for i in lone_dslfs:\n",
    "            for j in des_dslfs:\n",
    "                if pyrosetta.rosetta.core.conformation.is_disulfide_bond(\n",
    "                    despose.conformation(), i, j\n",
    "                ):\n",
    "                    pyrosetta.rosetta.core.conformation.break_disulfide(\n",
    "                        despose.conformation(), i, j\n",
    "                    )\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        if design_sel is not None:\n",
    "            designable = ResidueIndexSelector(design_sel)\n",
    "        else:\n",
    "            if neighbors:  # design neighbors too\n",
    "                designable = NeighborhoodResidueSelector(\n",
    "                    ResidueIndexSelector(diff),\n",
    "                    6,\n",
    "                    True,\n",
    "                )\n",
    "            else:  # design only diff\n",
    "                designable = ResidueIndexSelector(diff)\n",
    "        packable = NeighborhoodResidueSelector(designable, 6, True)\n",
    "        pack_option = RestrictToRepackingRLT()\n",
    "        pack = OperateOnResidueSubset(pack_option, designable, True)\n",
    "        lock_option = PreventRepackingRLT()\n",
    "        lock = OperateOnResidueSubset(lock_option, packable, True)\n",
    "        # add standard task operations\n",
    "        arochi = pyrosetta.rosetta.protocols.task_operations.LimitAromaChi2Operation()\n",
    "        arochi.chi2max(110)\n",
    "        arochi.chi2min(70)\n",
    "        arochi.include_trp(True)\n",
    "        ifcl = pyrosetta.rosetta.core.pack.task.operation.InitializeFromCommandline()\n",
    "        # setup custom layer design\n",
    "        ss1 = pyrosetta.rosetta.core.scoring.dssp.Dssp(state_X)\n",
    "        ss2 = pyrosetta.rosetta.core.scoring.dssp.Dssp(state_Y)\n",
    "        surf_sel = LayerSelector()\n",
    "        surf_sel.set_layers(0, 0, 1)\n",
    "        surf_sel.set_use_sc_neighbors(0)\n",
    "        surf_sel.set_cutoffs(20, 50)\n",
    "        surf1 = get_residues_from_subset(surf_sel.apply(state_X))\n",
    "        surf2 = get_residues_from_subset(surf_sel.apply(state_Y))\n",
    "        core_sel = LayerSelector()\n",
    "        core_sel.set_layers(1, 0, 0)\n",
    "        core_sel.set_use_sc_neighbors(0)\n",
    "        core1 = get_residues_from_subset(core_sel.apply(state_X))\n",
    "        core2 = get_residues_from_subset(core_sel.apply(state_Y))\n",
    "\n",
    "        intf_layr = pyrosetta.rosetta.utility.vector1_unsigned_long()\n",
    "        core_both = pyrosetta.rosetta.utility.vector1_unsigned_long()\n",
    "        surf_both = pyrosetta.rosetta.utility.vector1_unsigned_long()\n",
    "        bdry_core = pyrosetta.rosetta.utility.vector1_unsigned_long()\n",
    "        bdry_surf = pyrosetta.rosetta.utility.vector1_unsigned_long()\n",
    "        surf_core = pyrosetta.rosetta.utility.vector1_unsigned_long()\n",
    "        bdry_both = pyrosetta.rosetta.utility.vector1_unsigned_long()\n",
    "        # enumerate all 9 possible combinations + hh_int\n",
    "        for i in allres:\n",
    "            if i in hh_int:\n",
    "                intf_layr.append(i)\n",
    "\n",
    "            elif i in core1:\n",
    "                if i in core2:\n",
    "                    core_both.append(i)\n",
    "                elif i in surf2:\n",
    "                    surf_core.append(i)\n",
    "                else:\n",
    "                    bdry_core.append(i)\n",
    "            elif i in surf1:\n",
    "                if i in surf2:\n",
    "                    surf_both.append(i)\n",
    "                elif i in core2:\n",
    "                    surf_core.append(i)\n",
    "                else:\n",
    "                    bdry_surf.append(i)\n",
    "            else:\n",
    "                if i in core2:\n",
    "                    bdry_core.append(i)\n",
    "                elif i in surf2:\n",
    "                    bdry_surf.append(i)\n",
    "                else:\n",
    "                    bdry_both.append(i)\n",
    "\n",
    "        if len(intf_layr) > 0:\n",
    "            sel_intf_layr = ResidueIndexSelector(intf_layr)\n",
    "        else:\n",
    "            sel_intf_layr = FalseResidueSelector()\n",
    "        if len(core_both) > 0:\n",
    "            sel_core_both = ResidueIndexSelector(core_both)\n",
    "        else:\n",
    "            sel_core_both = FalseResidueSelector()\n",
    "        sel_surf_both = ResidueIndexSelector(surf_both)\n",
    "        if len(bdry_core) > 0:\n",
    "            sel_bdry_core = ResidueIndexSelector(bdry_core)\n",
    "        else:\n",
    "            sel_bdry_core = FalseResidueSelector()\n",
    "        if len(bdry_surf) > 0:\n",
    "            sel_bdry_surf = ResidueIndexSelector(bdry_surf)\n",
    "        else:\n",
    "            sel_bdry_surf = FalseResidueSelector()\n",
    "        if len(surf_core) > 0:\n",
    "            sel_surf_core = ResidueIndexSelector(surf_core)\n",
    "        else:\n",
    "            sel_surf_core = FalseResidueSelector()\n",
    "        sel_bdry_both = ResidueIndexSelector(bdry_both)\n",
    "        if strict_layers:\n",
    "            sel_c = OrResidueSelector(sel_core_both, sel_bdry_core)\n",
    "            sel_b = OrResidueSelector(sel_bdry_both, sel_surf_core)\n",
    "            sel_s = OrResidueSelector(sel_surf_both, sel_bdry_surf)\n",
    "        else:\n",
    "            sel_c = sel_core_both\n",
    "            sel_s = sel_surf_both\n",
    "            sel_c_or_s = OrResidueSelector(sel_core_both, sel_surf_both)\n",
    "            sel_b = NotResidueSelector(sel_c_or_s)\n",
    "\n",
    "        objs_sel = pyrosetta.rosetta.protocols.rosetta_scripts.XmlObjects.create_from_string(\n",
    "            \"\"\"\n",
    "            <RESIDUE_SELECTORS>\n",
    "                <SecondaryStructure name=\"sheet\" overlap=\"0\" minH=\"3\" minE=\"2\" include_terminal_loops=\"false\" use_dssp=\"true\" ss=\"E\"/>\n",
    "                <SecondaryStructure name=\"entire_loop\" overlap=\"0\" minH=\"3\" minE=\"2\" include_terminal_loops=\"true\" use_dssp=\"true\" ss=\"L\"/>\n",
    "                <SecondaryStructure name=\"entire_helix\" overlap=\"0\" minH=\"3\" minE=\"2\" include_terminal_loops=\"false\" use_dssp=\"true\" ss=\"H\"/>\n",
    "                <And name=\"helix_cap\" selectors=\"entire_loop\">\n",
    "                    <PrimarySequenceNeighborhood lower=\"1\" upper=\"0\" selector=\"entire_helix\"/>\n",
    "                </And>\n",
    "                <And name=\"helix_start\" selectors=\"entire_helix\">\n",
    "                    <PrimarySequenceNeighborhood lower=\"0\" upper=\"1\" selector=\"helix_cap\"/>\n",
    "                </And>\n",
    "                <And name=\"helix\" selectors=\"entire_helix\">\n",
    "                    <Not selector=\"helix_start\"/>\n",
    "                </And>\n",
    "                <And name=\"loop\" selectors=\"entire_loop\">\n",
    "                    <Not selector=\"helix_cap\"/>\n",
    "                </And>\n",
    "            </RESIDUE_SELECTORS>\n",
    "            \"\"\"\n",
    "        )\n",
    "        helix_sel = objs_sel.get_residue_selector(\"helix\")\n",
    "        loop_sel = objs_sel.get_residue_selector(\"loop\")\n",
    "        helix_cap_sel = objs_sel.get_residue_selector(\"helix_cap\")\n",
    "\n",
    "        core_hlx_sel = AndResidueSelector(sel_c, helix_sel)\n",
    "        bdry_hlx_sel = AndResidueSelector(sel_b, helix_sel)\n",
    "        surf_hlx_sel = AndResidueSelector(sel_s, helix_sel)\n",
    "        core_loop_sel = AndResidueSelector(sel_c, loop_sel)\n",
    "        bdry_loop_sel = AndResidueSelector(sel_b, loop_sel)\n",
    "        surf_loop_sel = AndResidueSelector(sel_s, loop_sel)\n",
    "\n",
    "        # layer design task ops, allows the current residue at a given position if it is not included\n",
    "        intf_layr_task = RestrictAbsentCanonicalAASExceptNativeRLT()\n",
    "        intf_layr_task.aas_to_keep(\"AEFHIKLNQRSTVWYM\")\n",
    "        core_hlx_task = RestrictAbsentCanonicalAASExceptNativeRLT()\n",
    "        core_hlx_task.aas_to_keep(\"AFILVW\")\n",
    "        bdry_hlx_task = RestrictAbsentCanonicalAASExceptNativeRLT()\n",
    "        bdry_hlx_task.aas_to_keep(\"AEHIKLNQRSTVWYM\")\n",
    "        surf_hlx_task = RestrictAbsentCanonicalAASExceptNativeRLT()\n",
    "        surf_hlx_task.aas_to_keep(\"EHKQR\")\n",
    "        core_loop_task = RestrictAbsentCanonicalAASExceptNativeRLT()\n",
    "        core_loop_task.aas_to_keep(\"AFGILPVW\")\n",
    "        bdry_loop_task = RestrictAbsentCanonicalAASExceptNativeRLT()\n",
    "        bdry_loop_task.aas_to_keep(\"ADEFGHIKLNPQRSTVWY\")\n",
    "        surf_loop_task = RestrictAbsentCanonicalAASExceptNativeRLT()\n",
    "        surf_loop_task.aas_to_keep(\"DEGHKNPQRST\")\n",
    "        hlx_cap_task = RestrictAbsentCanonicalAASExceptNativeRLT()\n",
    "        hlx_cap_task.aas_to_keep(\"DNSTP\")\n",
    "\n",
    "        intf_layr_op = OperateOnResidueSubset(intf_layr_task, sel_intf_layr, False)\n",
    "        hlx_cap_op = OperateOnResidueSubset(hlx_cap_task, helix_cap_sel, False)\n",
    "        core_hlx_op = OperateOnResidueSubset(core_hlx_task, core_hlx_sel, False)\n",
    "        bdry_hlx_op = OperateOnResidueSubset(bdry_hlx_task, bdry_hlx_sel, False)\n",
    "        surf_hlx_op = OperateOnResidueSubset(surf_hlx_task, surf_hlx_sel, False)\n",
    "        core_loop_op = OperateOnResidueSubset(core_loop_task, core_loop_sel, False)\n",
    "        bdry_loop_op = OperateOnResidueSubset(bdry_loop_task, bdry_loop_sel, False)\n",
    "        surf_loop_op = OperateOnResidueSubset(surf_loop_task, surf_loop_sel, False)\n",
    "\n",
    "        # push back all task ops, assumes no sheets\n",
    "        task_factory = pyrosetta.rosetta.core.pack.task.TaskFactory()\n",
    "        task_factory.push_back(pack)\n",
    "        task_factory.push_back(lock)\n",
    "        task_factory.push_back(arochi)\n",
    "        task_factory.push_back(ifcl)\n",
    "        task_factory.push_back(intf_layr_op)\n",
    "        task_factory.push_back(hlx_cap_op)\n",
    "        task_factory.push_back(core_hlx_op)\n",
    "        task_factory.push_back(bdry_hlx_op)\n",
    "        task_factory.push_back(surf_hlx_op)\n",
    "        task_factory.push_back(core_loop_op)\n",
    "        task_factory.push_back(bdry_loop_op)\n",
    "        task_factory.push_back(surf_loop_op)\n",
    "\n",
    "        # add design movers\n",
    "        objs = pyrosetta.rosetta.protocols.rosetta_scripts.XmlObjects.create_from_string(\n",
    "            \"\"\"\n",
    "            <MOVERS>\n",
    "            <FastDesign name=\"fastdesign\" repeats=\"1\" relaxscript=\"InterfaceDesign2019\"\n",
    "                cartesian=\"false\" dualspace=\"false\" ramp_down_constraints=\"false\"\n",
    "                bondangle=\"false\" bondlength=\"false\" min_type=\"lbfgs_armijo_nonmonotone\">\n",
    "            </FastDesign>\n",
    "            <AddSapConstraintMover name=\"add_sap\" speed=\"lightning\" sap_goal=\"0\" penalty_per_sap=\"{np_penalty}\" />\n",
    "            <AddCompositionConstraintMover name=\"ala_pen\" >\n",
    "                <Comp entry=\"PENALTY_DEFINITION;TYPE ALA;ABSOLUTE 0;PENALTIES 0 {ala_penalty};DELTA_START 0;DELTA_END 1;BEFORE_FUNCTION CONSTANT;AFTER_FUNCTION LINEAR;END_PENALTY_DEFINITION;\" />\n",
    "            </AddCompositionConstraintMover>\n",
    "            </MOVERS>\n",
    "            \"\"\".format(\n",
    "                np_penalty=np_penalty, ala_penalty=ala_penalty\n",
    "            )\n",
    "        )\n",
    "        surfpol = objs.get_mover(\"add_sap\")\n",
    "        surfpol.apply(despose)\n",
    "        ala_pen = objs.get_mover(\"ala_pen\")\n",
    "        ala_pen.apply(despose)\n",
    "        fast_design = objs.get_mover(\"fastdesign\")\n",
    "        fast_design.set_scorefxn(sfxn)\n",
    "        fast_design.set_task_factory(task_factory)\n",
    "        # skip design if sequences have already converged\n",
    "        if len(diff) > 0:\n",
    "            pyrosetta.rosetta.protocols.protein_interface_design.FavorNativeResidue(\n",
    "                despose, weight\n",
    "            )\n",
    "            fast_design.apply(despose)\n",
    "        # remove constraints\n",
    "        clear_constraints = (\n",
    "            pyrosetta.rosetta.protocols.constraint_movers.ClearConstraintsMover()\n",
    "        )\n",
    "        clear_constraints.apply(despose)\n",
    "        return\n",
    "\n",
    "    # recover original interfacial residues and combine those from each state, assumes middle split\n",
    "    objs_sse = pyrosetta.rosetta.protocols.rosetta_scripts.XmlObjects.create_from_string(\n",
    "        \"\"\"\n",
    "        <RESIDUE_SELECTORS>\n",
    "            <SSElement name=\"part1\" selection=\"n_term\" to_selection=\"{pre},H,E\" chain=\"A\" reassign_short_terminal_loop=\"2\" />\n",
    "            <SSElement name=\"part2\" selection=\"-{post},H,S\" to_selection=\"c_term\" chain=\"A\" reassign_short_terminal_loop=\"2\" />\n",
    "        </RESIDUE_SELECTORS>\n",
    "        \"\"\".format(\n",
    "            pre=int(scores[\"pre_break_helix\"]),\n",
    "            post=int(scores[\"pre_break_helix\"]),\n",
    "        )\n",
    "    )\n",
    "    part1 = objs_sse.get_residue_selector(\"part1\")\n",
    "    part2 = objs_sse.get_residue_selector(\"part2\")\n",
    "    intsel = InterGroupInterfaceByVectorSelector(part1, part2)\n",
    "    intdes = get_residues_from_subset(intsel.apply(state_Y))\n",
    "    intref = get_residues_from_subset(intsel.apply(state_X))\n",
    "    intall = pyrosetta.rosetta.utility.vector1_unsigned_long()\n",
    "    # add all residues in either interface to be designed\n",
    "    for i in intdes:\n",
    "        intall.append(i)\n",
    "    for i in intref:\n",
    "        intall.append(i)\n",
    "    # one round msd with no weight, lenient layers, no neighbors on all residues that are interface in either state\n",
    "    msd_fnr(\n",
    "        despose=state_Y,\n",
    "        refpose=state_X,\n",
    "        weight=0,\n",
    "        strict_layers=False,\n",
    "        neighbors=False,\n",
    "        design_sel=intall,\n",
    "        design_helix=False,\n",
    "    )\n",
    "    # one round msd with no weight, strict layers, and neighbors on all residues that are different between states\n",
    "    msd_fnr(\n",
    "        despose=state_X,\n",
    "        refpose=state_Y,\n",
    "        weight=0,\n",
    "        strict_layers=True,\n",
    "        neighbors=True,\n",
    "        design_helix=False,\n",
    "    )\n",
    "    # one round msd with no weight, strict layers, no neighbors on all residues that are different between states\n",
    "    msd_fnr(\n",
    "        despose=state_Y,\n",
    "        refpose=state_X,\n",
    "        weight=0,\n",
    "        strict_layers=True,\n",
    "        design_helix=True,\n",
    "    )\n",
    "    # two rounds, ramp weight with strict layers, no neighbors on all residues that are different between states\n",
    "    for wt in [0.2, 0.5, 1.0]:\n",
    "        msd_fnr(\n",
    "            despose=state_X,\n",
    "            refpose=state_Y,\n",
    "            weight=wt,\n",
    "            strict_layers=True,\n",
    "            design_helix=False,\n",
    "        )\n",
    "        msd_fnr(\n",
    "            despose=state_Y,\n",
    "            refpose=state_X,\n",
    "            weight=wt,\n",
    "            strict_layers=True,\n",
    "            design_helix=True,\n",
    "        )\n",
    "    # two rounds, ramp weight with lenient layers, no neighbors on all residues that are different between states\n",
    "    for wt in [1.5, 2.0]:\n",
    "        msd_fnr(\n",
    "            despose=state_X,\n",
    "            refpose=state_Y,\n",
    "            weight=wt,\n",
    "            strict_layers=False,\n",
    "            design_helix=False,\n",
    "        )\n",
    "        msd_fnr(\n",
    "            despose=state_Y,\n",
    "            refpose=state_X,\n",
    "            weight=wt,\n",
    "            strict_layers=False,\n",
    "            design_helix=True,\n",
    "        )\n",
    "    # set SAP penalty to 1 and alanine penalty for 0 for the last rounds\n",
    "    np_penalty = 1\n",
    "    ala_penalty = 0\n",
    "    wt = 10\n",
    "    # four rounds, max weight with lenient layers, no neighbors on all residues that are different between states\n",
    "    msd_fnr(\n",
    "        despose=state_X,\n",
    "        refpose=state_Y,\n",
    "        weight=wt,\n",
    "        strict_layers=False,\n",
    "        design_helix=False,\n",
    "    )\n",
    "    msd_fnr(\n",
    "        despose=state_Y,\n",
    "        refpose=state_X,\n",
    "        weight=wt,\n",
    "        strict_layers=False,\n",
    "        design_helix=False,\n",
    "    )\n",
    "    wt = 100  # force convergence\n",
    "    msd_fnr(\n",
    "        despose=state_X,\n",
    "        refpose=state_Y,\n",
    "        weight=wt,\n",
    "        strict_layers=False,\n",
    "        design_helix=False,\n",
    "    )\n",
    "\n",
    "    # if sequences fail to converge, report failure and do not yield combined pose\n",
    "    try:\n",
    "        assert state_X.sequence() == state_Y.sequence()\n",
    "    except AssertionError:\n",
    "        print(\"Convergence failure with the following sequences:\")\n",
    "        print(\"X:\", state_X.sequence())\n",
    "        print(\"Y:\", state_Y.sequence())\n",
    "        return\n",
    "    combined_scores = {}\n",
    "    combined_scores[\"closure_type_X\"] = scores_X[\"closure_type\"]\n",
    "    combined_scores[\"closure_type_Y\"] = scores_Y[\"closure_type\"]\n",
    "    combined_scores[\"disulfide_at_X\"] = scores_X[\"disulfide_at\"]\n",
    "    combined_scores[\"disulfide_at_Y\"] = scores_Y[\"disulfide_at\"]\n",
    "    combined_scores[\"dslf_fa13_cart_X\"] = scores_X[\"dslf_fa13_cart\"]\n",
    "    combined_scores[\"dslf_fa13_cart_Y\"] = scores_Y[\"dslf_fa13_cart\"]\n",
    "    combined_scores[\"rmsd_cart_X\"] = scores_X[\"rmsd_cart\"]\n",
    "    combined_scores[\"rmsd_cart_Y\"] = scores_Y[\"rmsd_cart\"]\n",
    "    combined_scores[\"parent_sequence\"] = parent_sequence\n",
    "    combined_scores[\"ala_penalty\"] = og_ala_penalty\n",
    "    combined_scores[\"np_penalty\"] = og_np_penalty\n",
    "    common_keys = [\n",
    "        \"new_loop_resis\",\n",
    "        \"parent\",\n",
    "        \"bb_clash\",\n",
    "        \"pivot_helix\",\n",
    "        \"pre_break_helix\",\n",
    "        \"shift\",\n",
    "        \"total_length\",\n",
    "    ]\n",
    "    for common_key in common_keys:\n",
    "        combined_scores[common_key] = scores[common_key]\n",
    "    # make base scoring xml\n",
    "    xml_base = \"\"\"\n",
    "    <ROSETTASCRIPTS>\n",
    "        <SCOREFXNS>\n",
    "            <ScoreFunction name=\"sfxn\" weights=\"beta_nov16\" />\n",
    "            <ScoreFunction name=\"sfxn_design\" weights=\"beta_nov16\" >\n",
    "                <Set use_hb_env_dep=\"true\" />\n",
    "                <Reweight scoretype=\"approximate_buried_unsat_penalty\" weight=\"17\" />\n",
    "                <Set approximate_buried_unsat_penalty_burial_atomic_depth=\"3.5\" />\n",
    "                <Set approximate_buried_unsat_penalty_hbond_energy_threshold=\"-1.0\" />\n",
    "                <Set approximate_buried_unsat_penalty_natural_corrections1=\"true\" />\n",
    "                <Set approximate_buried_unsat_penalty_hbond_bonus_cross_chain=\"-7\" />\n",
    "                <Set approximate_buried_unsat_penalty_hbond_bonus_ser_to_helix_bb=\"1\"/>                    \n",
    "            </ScoreFunction>\n",
    "        </SCOREFXNS>\n",
    "        <RESIDUE_SELECTORS>\n",
    "            <Chain name=\"chA\" chains=\"A\"/>\n",
    "            <Chain name=\"chB\" chains=\"B\"/>\n",
    "            <Index name=\"new_loop_resis\" resnums=\"{new_loop_resis}\" />\n",
    "            <Neighborhood name=\"around_new_loop\" selector=\"new_loop_resis\" distance=\"8.0\" />\n",
    "            <SSElement name=\"part1\" selection=\"n_term\" to_selection=\"{pre},H,E\" chain=\"A\" reassign_short_terminal_loop=\"2\" />\n",
    "            <SSElement name=\"mid\" selection=\"{pre},H,E\" to_selection=\"-{post},H,S\" chain=\"A\" reassign_short_terminal_loop=\"2\" />\n",
    "            <SSElement name=\"part2\" selection=\"-{post},H,S\" to_selection=\"c_term\" chain=\"A\" reassign_short_terminal_loop=\"2\" />\n",
    "            <Or name=\"chB_OR_mid\" selectors=\"chB,mid\" />\n",
    "        </RESIDUE_SELECTORS>\n",
    "        <TASKOPERATIONS>\n",
    "            <IncludeCurrent name=\"current\" />\n",
    "            <LimitAromaChi2 name=\"arochi\" chi2max=\"110\" chi2min=\"70\" include_trp=\"True\" />\n",
    "            <ExtraRotamersGeneric name=\"ex1_ex2\" ex1=\"1\" ex2=\"1\" />\n",
    "            <InitializeFromCommandline name=\"ifcl\"/>\n",
    "            <ProteinInterfaceDesign name=\"pack_long\" design_chain1=\"0\" design_chain2=\"0\" jump=\"1\" interface_distance_cutoff=\"15\"/>\n",
    "        </TASKOPERATIONS>\n",
    "        <MOVERS>\n",
    "            <SavePoseMover name=\"save_before_relax\" restore_pose=\"0\" reference_name=\"before_relax\"/>\n",
    "            <DeleteRegionMover name=\"cutn\" residue_selector=\"part1\" rechain=\"true\" />\n",
    "            <DeleteRegionMover name=\"cutc\" residue_selector=\"part2\" rechain=\"true\" />\n",
    "            <DeleteRegionMover name=\"cutB\" residue_selector=\"chB_OR_mid\" rechain=\"true\" />\n",
    "            <FastRelax name=\"relax\" scorefxn=\"sfxn_design\" repeats=\"1\" batch=\"false\" ramp_down_constraints=\"false\"\n",
    "                cartesian=\"false\" bondangle=\"false\" bondlength=\"false\" min_type=\"dfpmin_armijo_nonmonotone\"\n",
    "                task_operations=\"ifcl,current,arochi,ex1_ex2\" >\n",
    "            </FastRelax>\n",
    "            <TaskAwareMinMover name=\"min\" scorefxn=\"sfxn\" bb=\"0\" chi=\"1\" task_operations=\"pack_long\" />\n",
    "        </MOVERS>\n",
    "        <FILTERS>\n",
    "            <BuriedUnsatHbonds name=\"vbuns\" use_reporter_behavior=\"true\" report_all_heavy_atom_unsats=\"true\" \n",
    "                scorefxn=\"sfxn\" ignore_surface_res=\"false\" print_out_info_to_pdb=\"true\" confidence=\"0\" \n",
    "                use_ddG_style=\"false\" dalphaball_sasa=\"true\" probe_radius=\"1.1\" atomic_depth_selection=\"5.5\" \n",
    "                burial_cutoff=\"1000\" burial_cutoff_apo=\"0.2\" />\n",
    "            <BuriedUnsatHbonds name=\"sbuns\" use_reporter_behavior=\"true\" report_all_heavy_atom_unsats=\"true\"\n",
    "                scorefxn=\"sfxn\" ignore_surface_res=\"false\" print_out_info_to_pdb=\"true\" confidence=\"0\"\n",
    "                use_ddG_style=\"false\" burial_cutoff=\"0.01\" dalphaball_sasa=\"true\" probe_radius=\"1.1\" \n",
    "                atomic_depth_selection=\"5.5\" atomic_depth_deeper_than=\"false\" />\n",
    "            <BuriedUnsatHbonds name=\"buns\" use_reporter_behavior=\"true\" report_all_heavy_atom_unsats=\"true\" \n",
    "                scorefxn=\"sfxn\" ignore_surface_res=\"false\" print_out_info_to_pdb=\"true\" confidence=\"0\" \n",
    "                use_ddG_style=\"false\" burial_cutoff=\"0.01\" dalphaball_sasa=\"true\" probe_radius=\"1.1\"\n",
    "                max_hbond_energy=\"1.5\" burial_cutoff_apo=\"0.2\" />\n",
    "            <SSShapeComplementarity name=\"sc\" verbose=\"1\" loops=\"1\" helices=\"1\" />\n",
    "            <TaskAwareScoreType name=\"tot_score\" scorefxn=\"sfxn\" score_type=\"total_score\" threshold=\"0\" mode=\"total\"  confidence=\"0\" />\n",
    "            <MoveBeforeFilter name=\"score_nc\" mover=\"cutB\" filter=\"tot_score\" confidence=\"0\" />\n",
    "            <MoveBeforeFilter name=\"score_nB\" mover=\"cutc\" filter=\"tot_score\" confidence=\"0\" />\n",
    "            <MoveBeforeFilter name=\"score_Bc\" mover=\"cutn\" filter=\"tot_score\" confidence=\"0\" />\n",
    "            <ExposedHydrophobics name=\"exposed_hydrophobics\" />\n",
    "            <Geometry name=\"geometry\"\n",
    "                confidence=\"0\"\n",
    "                count_bad_residues=\"true\" />\n",
    "            <Geometry name=\"geometry_loop\" \n",
    "                residue_selector=\"around_new_loop\" \n",
    "                confidence=\"0\"\n",
    "                count_bad_residues=\"true\" />\n",
    "            <SSPrediction name=\"mismatch_probability\" confidence=\"0\" \n",
    "                cmd=\"/software/psipred4/runpsipred_single\" use_probability=\"1\" \n",
    "                mismatch_probability=\"1\" use_svm=\"1\" />\n",
    "            <Rmsd name=\"rmsd_final\" reference_name=\"before_relax\" chains=\"AB\" superimpose=\"1\" threshold=\"5\" by_aln=\"0\" confidence=\"0\" />\n",
    "            <ScoreType name=\"total_score_pose\" scorefxn=\"sfxn\" score_type=\"total_score\" threshold=\"0\" confidence=\"0\" />\n",
    "            <ResidueCount name=\"count\" />\n",
    "            <CalculatorFilter name=\"score_per_res\" equation=\"total_score_full / res\" threshold=\"-2.0\" confidence=\"0\">\n",
    "                <Var name=\"total_score_full\" filter=\"total_score_pose\"/>\n",
    "                <Var name=\"res\" filter=\"count\"/>\n",
    "            </CalculatorFilter>        \n",
    "            <worst9mer name=\"wnm_all\" rmsd_lookup_threshold=\"0.4\" confidence=\"0\" />\n",
    "            <worst9mer name=\"wnm_hlx\" rmsd_lookup_threshold=\"0.4\" confidence=\"0\" only_helices=\"true\" />\n",
    "    \"\"\".format(\n",
    "        new_loop_resis=new_loop_resis,\n",
    "        pre=int(scores[\"pre_break_helix\"]),\n",
    "        post=int(scores[\"pre_break_helix\"]),\n",
    "    )\n",
    "    # score state X\n",
    "    xml_X = (\n",
    "        xml_base\n",
    "        + \"\"\"\n",
    "            </FILTERS>\n",
    "            <SIMPLE_METRICS>\n",
    "                <SapScoreMetric name=\"sap_score\" />\n",
    "            </SIMPLE_METRICS>\n",
    "            <APPLY_TO_POSE>\n",
    "            </APPLY_TO_POSE>\n",
    "            <PROTOCOLS>\n",
    "                <Add mover_name=\"save_before_relax\" />\n",
    "                <Add mover_name=\"relax\"/>\n",
    "                <Add filter_name=\"buns\" />\n",
    "                <Add filter_name=\"sbuns\" />\n",
    "                <Add filter_name=\"vbuns\" />\n",
    "                <Add filter_name=\"exposed_hydrophobics\" />\n",
    "                <Add filter_name=\"geometry\"/>\n",
    "                <Add filter_name=\"geometry_loop\"/>\n",
    "                <Add filter_name=\"mismatch_probability\" />\n",
    "                <Add filter_name=\"rmsd_final\" />\n",
    "                <Add metrics=\"sap_score\" />\n",
    "                <Add filter_name=\"sc\" />\n",
    "                <Add filter_name=\"score_nc\" />\n",
    "                <Add filter_name=\"score_nB\" />\n",
    "                <Add filter_name=\"score_Bc\" />\n",
    "                <Add filter_name=\"score_per_res\" />\n",
    "                <Add filter_name=\"wnm_all\" />\n",
    "                <Add filter_name=\"wnm_hlx\" />\n",
    "            </PROTOCOLS>\n",
    "            <OUTPUT scorefxn=\"sfxn\" />\n",
    "        </ROSETTASCRIPTS>\n",
    "        \"\"\"\n",
    "    )\n",
    "    score_X = SingleoutputRosettaScriptsTask(xml_X)\n",
    "    scored_X = score_X(state_X.clone())\n",
    "    scores_X = scored_X.pose.scores\n",
    "    scores_X = {f\"{key}_X\": value for key, value in scores_X.items()}\n",
    "    # score state Y\n",
    "    xml_Y = (\n",
    "        xml_base\n",
    "        + \"\"\"\n",
    "                <ContactMolecularSurface name=\"cms\" target_selector=\"chA\" binder_selector=\"chB\" confidence=\"0\" />\n",
    "                <ContactMolecularSurface name=\"cms_nc\" target_selector=\"part1\" binder_selector=\"part2\" confidence=\"0\" />\n",
    "                <ContactMolecularSurface name=\"cms_nB\" target_selector=\"part1\" binder_selector=\"chB\" confidence=\"0\" />\n",
    "                <ContactMolecularSurface name=\"cms_Bc\" target_selector=\"part2\" binder_selector=\"chB\" confidence=\"0\" />\n",
    "                <Ddg name=\"ddg\" threshold=\"-10\" jump=\"1\" repeats=\"5\" repack=\"1\" relax_mover=\"min\" confidence=\"0\" scorefxn=\"sfxn\" />\n",
    "                <Sasa name=\"sasa\" confidence=\"0\" />\n",
    "                <MoveBeforeFilter name=\"sasa_nc\" mover=\"cutB\" filter=\"sasa\" confidence=\"0\" />\n",
    "                <MoveBeforeFilter name=\"sasa_nB\" mover=\"cutc\" filter=\"sasa\" confidence=\"0\" />\n",
    "                <MoveBeforeFilter name=\"sasa_Bc\" mover=\"cutn\" filter=\"sasa\" confidence=\"0\" />\n",
    "                <ShapeComplementarity name=\"sc_int\" verbose=\"0\" min_sc=\"0.55\" write_int_area=\"1\" write_median_dist=\"1\" jump=\"1\" confidence=\"0\"/>\n",
    "                <MoveBeforeFilter name=\"sc_int_nc\" mover=\"cutB\" filter=\"sc_int\" confidence=\"0\" />\n",
    "                <MoveBeforeFilter name=\"sc_int_nB\" mover=\"cutc\" filter=\"sc_int\" confidence=\"0\" />\n",
    "                <MoveBeforeFilter name=\"sc_int_Bc\" mover=\"cutn\" filter=\"sc_int\" confidence=\"0\" />\n",
    "\n",
    "            </FILTERS>\n",
    "            <SIMPLE_METRICS>\n",
    "                <SapScoreMetric name=\"sap_score\" />\n",
    "            </SIMPLE_METRICS>\n",
    "            <APPLY_TO_POSE>\n",
    "            </APPLY_TO_POSE>\n",
    "            <PROTOCOLS>\n",
    "                <Add mover_name=\"save_before_relax\" />\n",
    "                <Add mover_name=\"relax\"/>\n",
    "                <Add filter_name=\"buns\" />\n",
    "                <Add filter_name=\"sbuns\" />\n",
    "                <Add filter_name=\"vbuns\" />\n",
    "                <Add filter_name=\"cms\" />\n",
    "                <Add filter_name=\"cms_nc\" />\n",
    "                <Add filter_name=\"cms_nB\" />\n",
    "                <Add filter_name=\"cms_Bc\" />\n",
    "                <Add filter_name=\"ddg\" />\n",
    "                <Add filter_name=\"sasa\" />\n",
    "                Add filter_name=\"sasa_nc\" />\n",
    "                Add filter_name=\"sasa_nB\" />\n",
    "                Add filter_name=\"sasa_Bc\" />\n",
    "                <Add filter_name=\"tot_score\" />\n",
    "                <Add filter_name=\"score_nc\" />\n",
    "                <Add filter_name=\"score_nB\" />\n",
    "                <Add filter_name=\"score_Bc\" />\n",
    "                <Add filter_name=\"sc\" />\n",
    "                <Add filter_name=\"sc_int\" />\n",
    "                Add filter_name=\"sc_int_nc\" />\n",
    "                Add filter_name=\"sc_int_nB\" />\n",
    "                Add filter_name=\"sc_int_Bc\" />\n",
    "                <Add filter_name=\"exposed_hydrophobics\" />\n",
    "                <Add filter_name=\"geometry\"/>\n",
    "                <Add filter_name=\"geometry_loop\"/>\n",
    "                <Add filter_name=\"mismatch_probability\" />\n",
    "                <Add filter_name=\"rmsd_final\" />\n",
    "                <Add metrics=\"sap_score\" />\n",
    "                <Add filter_name=\"score_per_res\" />\n",
    "                <Add filter_name=\"wnm_all\" />\n",
    "                <Add filter_name=\"wnm_hlx\" />\n",
    "            </PROTOCOLS>\n",
    "            <OUTPUT scorefxn=\"sfxn\" />\n",
    "        </ROSETTASCRIPTS>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    score_Y = SingleoutputRosettaScriptsTask(xml_Y)\n",
    "    scored_Y = score_Y(state_Y.clone())\n",
    "    scores_Y = scored_Y.pose.scores\n",
    "    scores_Y = {f\"{key}_Y\": value for key, value in scores_Y.items()}\n",
    "\n",
    "    combined_XY = combined_pose_maker([state_X, state_Y])\n",
    "    sw = pyrosetta.rosetta.protocols.simple_moves.SwitchChainOrderMover()\n",
    "    sw.chain_order(\"1234\")\n",
    "    sw.apply(combined_XY)\n",
    "    combined_scores.update({**scores_X, **scores_Y})\n",
    "    # clear scores and update\n",
    "    pyrosetta.rosetta.core.pose.clearPoseExtraScores(combined_XY)\n",
    "    for key, value in combined_scores.items():\n",
    "        pyrosetta.rosetta.core.pose.setPoseExtraScore(combined_XY, key, value)\n",
    "    scored_ppose = io.to_pose(combined_XY)\n",
    "    return scored_ppose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup dask, set command line options, make tasks and submit to client again to test msd\n",
    "At some point I should maybe try using `client.wait_for_workers(n_workers=1, timeout=None)`  \n",
    "Using `ala_pen = 2`, `np_pen = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run the following from your local terminal:\n",
      "ssh -L 8000:localhost:8787 pleung@dig175\n",
      "Desigining 20 pairs\n",
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -e /mnt/home/pleung/logs/slurm_logs/dask-worker-%J.err\n",
      "#SBATCH -o /mnt/home/pleung/logs/slurm_logs/dask-worker-%J.out\n",
      "#SBATCH -p long\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=1\n",
      "#SBATCH --mem=4G\n",
      "#SBATCH -t 23:30:00\n",
      "\n",
      "JOB_ID=${SLURM_JOB_ID%;*}\n",
      "\n",
      "/home/pleung/.conda/envs/phil/bin/python -m distributed.cli.dask_worker tcp://172.16.131.12:33523 --nthreads 1 --memory-limit 3.73GiB --name name --nanny --death-timeout 120 --local-directory $TMPDIR/dask --lifetime 23h --lifetime-stagger 4m\n",
      "\n",
      "<Client: 'tcp://172.16.131.12:33523' processes=0 threads=0, memory=0 B>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyrosetta.distributed:maybe_init performing pyrosetta initialization: {'options': '-run:constant_seed 1 -multithreading:total_threads 1', 'extra_options': '-mute all', 'set_logging_handler': 'interactive', 'silent': True}\n",
      "INFO:pyrosetta.rosetta:Found rosetta database at: /home/pleung/.conda/envs/phil/lib/python3.8/site-packages/pyrosetta/database; using it....\n",
      "INFO:pyrosetta.rosetta:PyRosetta-4 2021 [Rosetta PyRosetta4.conda.linux.cxx11thread.serialization.CentOS.python38.Release 2021.12+release.ed6a5560506cfd327d4a6a3e2c9b0c9f6f4a6535 2021-03-26T16:09:25] retrieved from: http://www.pyrosetta.org\n",
      "(C) Copyright Rosetta Commons Member Institutions. Created in JHU by Sergey Lyskov and PyRosetta Team.\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from glob import glob\n",
    "import logging\n",
    "import pwd\n",
    "from pyrosetta.distributed.cluster.core import PyRosettaCluster\n",
    "\n",
    "\n",
    "print(\"run the following from your local terminal:\")\n",
    "print(\n",
    "    f\"ssh -L 8000:localhost:8787 {pwd.getpwuid(os.getuid()).pw_name}@{socket.gethostname()}\"\n",
    ")\n",
    "\n",
    "ala_pen = 2\n",
    "np_pen = 1\n",
    "\n",
    "\n",
    "def create_tasks(selected, options):\n",
    "    for pair in selected:\n",
    "        with open(pair, \"r\") as f:\n",
    "            for file in f:\n",
    "                paths = file.rstrip().split()\n",
    "                tasks = {\"options\": \"-corrections::beta_nov16 true\"}\n",
    "                tasks[\"extra_options\"] = options\n",
    "                tasks[\"-s\"] = paths[0]\n",
    "                tasks[\"-x\"] = paths[1]\n",
    "                tasks[\"ala_pen\"] = ala_pen\n",
    "                tasks[\"np_pen\"] = np_pen\n",
    "                yield tasks\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "pairs = glob(os.path.join(os.getcwd(), \"04_pairs/paired/DHR*/*.pair\"))\n",
    "hDHRS = glob(os.path.join(os.getcwd(), \"04_pairs/paired/hDHR*/*.pair\"))\n",
    "TH_DHRS = glob(os.path.join(os.getcwd(), \"04_pairs/paired/TH_DHR*/*.pair\"))\n",
    "TH_DHRS = [t for t in TH_DHRS if (\"_S2\" not in t and \"_C2\" not in t and \"_C9\" not in t)]\n",
    "hTH_DHRS = glob(os.path.join(os.getcwd(), \"04_pairs/paired/hTH_DHR*/*.pair\"))\n",
    "KH_DHRS = glob(os.path.join(os.getcwd(), \"04_pairs/paired/KH*/*.pair\"))\n",
    "selected = pairs + hDHRS + TH_DHRS + hTH_DHRS + KH_DHRS\n",
    "\n",
    "print(f\"Desigining {len(selected)} pairs\")\n",
    "\n",
    "options = {\n",
    "    \"-out:level\": \"300\",\n",
    "    \"-holes:dalphaball\": \"/home/bcov/ppi/tutorial_build/main/source/external/DAlpahBall/DAlphaBall.gcc\",\n",
    "    \"-indexed_structure_store:fragment_store\": \"/net/databases/VALL_clustered/connect_chains/ss_grouped_vall_helix_shortLoop.h5\",\n",
    "}\n",
    "\n",
    "output_path = os.path.join(os.getcwd(), \"05_msd_test\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # configure SLURM cluster as a context manager\n",
    "    with SLURMCluster(\n",
    "        cores=1,\n",
    "        processes=1,\n",
    "        job_cpu=1,\n",
    "        memory=\"4GB\",\n",
    "        queue=\"medium\",\n",
    "        walltime=\"23:30:00\",\n",
    "        death_timeout=120,\n",
    "        local_directory=\"$TMPDIR/dask\",\n",
    "        log_directory=\"/mnt/home/pleung/logs/slurm_logs\",\n",
    "        extra=[\"--lifetime\", \"23h\", \"--lifetime-stagger\", \"4m\"],\n",
    "    ) as cluster:\n",
    "        print(cluster.job_script())\n",
    "        # scale between 1-1020 workers,\n",
    "        cluster.adapt(\n",
    "            minimum=1,\n",
    "            maximum=1020,\n",
    "            wait_count=400,  # Number of consecutive times that a worker should be suggested for removal it is removed\n",
    "            interval=\"5s\",  # Time between checks\n",
    "        )\n",
    "        # setup a client to interact with the cluster as a context manager\n",
    "        with Client(cluster) as client:\n",
    "            print(client)\n",
    "            PyRosettaCluster(\n",
    "                tasks=create_tasks(selected, options),\n",
    "                client=client,\n",
    "                scratch_dir=output_path,\n",
    "                output_path=output_path,\n",
    "                nstruct=1,\n",
    "            ).distribute(protocols=[msd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're going to do a huge run here, more than it makes sense to use dask for\n",
    "Will submit ~75k CPU hours worth of stuff to `short` and `medium` and ~150k to `backfill`\n",
    "Make `SLURM` array tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, stat, subprocess\n",
    "\n",
    "\n",
    "def create_tasks(selected):\n",
    "    for pair in selected:\n",
    "        with open(pair, \"r\") as f:\n",
    "            for file in f:\n",
    "                for ala_pen in 1, 2:\n",
    "                    for np_pen in 1, 2:\n",
    "                        tasks = {}\n",
    "                        paths = file.rstrip().split()\n",
    "                        tasks[\"-s\"] = paths[0]\n",
    "                        tasks[\"-x\"] = paths[1]\n",
    "                        tasks[\"-ala_pen\"] = ala_pen\n",
    "                        tasks[\"-np_pen\"] = np_pen\n",
    "                        yield tasks\n",
    "\n",
    "\n",
    "pairs = glob(os.path.join(os.getcwd(), \"04_pairs/paired/DHR*/*.pair\"))\n",
    "hDHRS = glob(os.path.join(os.getcwd(), \"04_pairs/paired/hDHR*/*.pair\"))\n",
    "TH_DHRS = glob(os.path.join(os.getcwd(), \"04_pairs/paired/TH_DHR*/*.pair\"))\n",
    "TH_DHRS = [t for t in TH_DHRS if (\"_S2\" not in t and \"_C2\" not in t and \"_C9\" not in t)]\n",
    "hTH_DHRS = glob(os.path.join(os.getcwd(), \"04_pairs/paired/hTH_DHR*/*.pair\"))\n",
    "selected = pairs + hDHRS + TH_DHRS + hTH_DHRS\n",
    "\n",
    "\n",
    "msd_py = os.path.join(os.getcwd(), \"msd.py\")\n",
    "\n",
    "jid = \"{SLURM_JOB_ID%;*}\"\n",
    "sid = \"{SLURM_ARRAY_TASK_ID}p\"\n",
    "\n",
    "for i, queue in [\n",
    "    (0, \"backfill\"),\n",
    "    (1, \"short\"),\n",
    "    (2, \"short\"),\n",
    "    (3, \"short\"),\n",
    "    (4, \"short\"),\n",
    "]:\n",
    "    tasklist = f\"05_msd_tasks{i}.cmds\"\n",
    "    run_sh = \"\"\"#!/usr/bin/env bash \\n#SBATCH -J m1n10n \\n#SBATCH -e /mnt/home/pleung/logs/slurm_logs/m1n10n-%J.err \\n#SBATCH -o /mnt/home/pleung/logs/slurm_logs/m1n10n-%J.out \\n#SBATCH -p {queue} \\n#SBATCH --mem=4G \\n\\nJOB_ID=${jid} \\nCMD=$(sed -n \"${sid}\" {tasklist}) \\necho \"${c}\" | bash\"\"\".format(\n",
    "        queue=queue, jid=jid, sid=sid, tasklist=tasklist, c=\"{CMD}\"\n",
    "    )\n",
    "    shell = f\"05_msd_run{i}.sh\"\n",
    "    with open(shell, \"w+\") as f:\n",
    "        print(run_sh, file=f)\n",
    "    st = os.stat(shell)\n",
    "    os.chmod(shell, st.st_mode | stat.S_IEXEC)\n",
    "    with open(f\"05_msd_tasks{i}.cmds\", \"w+\") as f:\n",
    "        for nstruct in range(0, 25):\n",
    "            outpath = os.path.join(os.getcwd(), f\"05_msd_runs_{i}\")\n",
    "            full_outpath = os.path.join(os.getcwd(), outpath, f\"{nstruct}\")\n",
    "            for tasks in create_tasks(selected):\n",
    "                args_ = \" \".join([\" \".join([k, str(v)]) for k, v in tasks.items()])\n",
    "                cmd = f\"mkdir -p {full_outpath}; cd {full_outpath}; {msd_py} {args_}\"\n",
    "                print(cmd, file=f)\n",
    "\n",
    "# Let's go\n",
    "print(\"Run the following commands\")\n",
    "for i in \"01234\":\n",
    "    print(f\"sbatch -a 1-$(cat 05_msd_tasks{i}.cmds | wc -l) 05_msd_run{i}.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get reference values for state X\n",
    "for filtering purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrosetta.distributed import requires_init\n",
    "from pyrosetta.distributed.packed_pose.core import PackedPose\n",
    "\n",
    "\n",
    "@requires_init\n",
    "def score_ref(packed_pose_in: PackedPose, **kwargs) -> PackedPose:\n",
    "    \"\"\"\n",
    "    Score the parents for purposes of comparison\n",
    "    \"\"\"\n",
    "    import pyrosetta\n",
    "    import pyrosetta.distributed.io as io\n",
    "    from pyrosetta.distributed.tasks.rosetta_scripts import (\n",
    "        SingleoutputRosettaScriptsTask,\n",
    "    )\n",
    "\n",
    "    if packed_pose_in == None:\n",
    "        packed_pose_in = io.pose_from_file(kwargs[\"-s\"])\n",
    "    else:\n",
    "        raise RuntimeError\n",
    "    parent = kwargs[\"-s\"].split(\"/\")[-1].replace(\".pdb\", \"\")\n",
    "    pre_break_helix = kwargs[\"pre_break_helix\"]\n",
    "    sfxn = \"beta_nov16\"\n",
    "    xml = \"\"\"\n",
    "        <ROSETTASCRIPTS>\n",
    "            <SCOREFXNS>\n",
    "                <ScoreFunction name=\"sfxn\" weights=\"{sfxn}\" />\n",
    "                <ScoreFunction name=\"sfxn_design\" weights=\"{sfxn}\" >\n",
    "                    <Set use_hb_env_dep=\"true\" />\n",
    "                    <Reweight scoretype=\"approximate_buried_unsat_penalty\" weight=\"17\" />\n",
    "                    <Set approximate_buried_unsat_penalty_burial_atomic_depth=\"3.5\" />\n",
    "                    <Set approximate_buried_unsat_penalty_hbond_energy_threshold=\"-1.0\" />\n",
    "                    <Set approximate_buried_unsat_penalty_natural_corrections1=\"true\" />\n",
    "                    <Set approximate_buried_unsat_penalty_hbond_bonus_cross_chain=\"-7\" />\n",
    "                    <Set approximate_buried_unsat_penalty_hbond_bonus_ser_to_helix_bb=\"1\"/>                    \n",
    "                </ScoreFunction>\n",
    "            </SCOREFXNS>\n",
    "            <RESIDUE_SELECTORS>\n",
    "                <SSElement name=\"part1\" selection=\"n_term\" to_selection=\"{pre},H,E\" chain=\"A\" reassign_short_terminal_loop=\"2\" />\n",
    "                <SSElement name=\"part2\" selection=\"-{post},H,S\" to_selection=\"c_term\" chain=\"A\" reassign_short_terminal_loop=\"2\" />\n",
    "            </RESIDUE_SELECTORS>\n",
    "            <TASKOPERATIONS>\n",
    "                <IncludeCurrent name=\"current\" />\n",
    "                <LimitAromaChi2 name=\"arochi\" chi2max=\"110\" chi2min=\"70\" include_trp=\"True\" />\n",
    "                <ExtraRotamersGeneric name=\"ex1_ex2\" ex1=\"1\" ex2=\"1\" />\n",
    "                <InitializeFromCommandline name=\"ifcl\"/>\n",
    "            </TASKOPERATIONS>\n",
    "            <MOVERS>\n",
    "                <SavePoseMover name=\"save_before_relax\" restore_pose=\"0\" reference_name=\"before_relax\"/>\n",
    "            </MOVERS>\n",
    "            <FILTERS>\n",
    "                <BuriedUnsatHbonds name=\"vbuns\" use_reporter_behavior=\"true\" report_all_heavy_atom_unsats=\"true\" \n",
    "                    scorefxn=\"sfxn\" ignore_surface_res=\"false\" print_out_info_to_pdb=\"true\" confidence=\"0\" \n",
    "                    use_ddG_style=\"false\" dalphaball_sasa=\"true\" probe_radius=\"1.1\" atomic_depth_selection=\"5.5\" \n",
    "                    burial_cutoff=\"1000\" burial_cutoff_apo=\"0.2\" />\n",
    "                <BuriedUnsatHbonds name=\"sbuns\" use_reporter_behavior=\"true\" report_all_heavy_atom_unsats=\"true\"\n",
    "                    scorefxn=\"sfxn\" ignore_surface_res=\"false\" print_out_info_to_pdb=\"true\" confidence=\"0\"\n",
    "                    use_ddG_style=\"false\" burial_cutoff=\"0.01\" dalphaball_sasa=\"true\" probe_radius=\"1.1\" \n",
    "                    atomic_depth_selection=\"5.5\" atomic_depth_deeper_than=\"false\" />\n",
    "                <BuriedUnsatHbonds name=\"buns\" use_reporter_behavior=\"true\" report_all_heavy_atom_unsats=\"true\" \n",
    "                    scorefxn=\"sfxn\" ignore_surface_res=\"false\" print_out_info_to_pdb=\"true\" confidence=\"0\" \n",
    "                    use_ddG_style=\"false\" burial_cutoff=\"0.01\" dalphaball_sasa=\"true\" probe_radius=\"1.1\"\n",
    "                    max_hbond_energy=\"1.5\" burial_cutoff_apo=\"0.2\" />\n",
    "                <ContactMolecularSurface name=\"cms\" verbose=\"true\" target_selector=\"part1\" binder_selector=\"part2\"/>\n",
    "                <ExposedHydrophobics name=\"exposed_hydrophobics\" />\n",
    "                <Geometry name=\"geometry\"\n",
    "                    confidence=\"0\"\n",
    "                    count_bad_residues=\"true\" />\n",
    "                <SSPrediction name=\"mismatch_probability\" confidence=\"0\" \n",
    "                    cmd=\"/software/psipred4/runpsipred_single\" use_probability=\"1\" \n",
    "                    mismatch_probability=\"1\" use_svm=\"1\" />\n",
    "                <Rmsd name=\"rmsd_final\" reference_name=\"before_relax\" chains=\"A\" superimpose=\"1\" threshold=\"5\" by_aln=\"0\" confidence=\"0\" />\n",
    "                <ScoreType name=\"total_score_pose\" scorefxn=\"sfxn\" score_type=\"total_score\" threshold=\"0\" confidence=\"0\" />\n",
    "                <ResidueCount name=\"count\" />\n",
    "                <CalculatorFilter name=\"score_per_res\" equation=\"total_score_full / res\" threshold=\"-2.0\" confidence=\"0\">\n",
    "                    <Var name=\"total_score_full\" filter=\"total_score_pose\"/>\n",
    "                    <Var name=\"res\" filter=\"count\"/>\n",
    "                </CalculatorFilter>        \n",
    "                <worst9mer name=\"wnm_all\" rmsd_lookup_threshold=\"0.4\" confidence=\"0\" />\n",
    "                <worst9mer name=\"wnm_hlx\" rmsd_lookup_threshold=\"0.4\" confidence=\"0\" only_helices=\"true\" />\n",
    "\n",
    "            </FILTERS>\n",
    "            <MOVERS>\n",
    "                <FastRelax name=\"relax\" scorefxn=\"sfxn_design\" repeats=\"1\" batch=\"false\" ramp_down_constraints=\"false\"\n",
    "                    cartesian=\"false\" bondangle=\"false\" bondlength=\"false\" min_type=\"dfpmin_armijo_nonmonotone\"\n",
    "                    task_operations=\"ifcl,current,arochi,ex1_ex2\" >\n",
    "                </FastRelax>\n",
    "            </MOVERS>\n",
    "            <SIMPLE_METRICS>\n",
    "                <SapScoreMetric name=\"sap_score\" />\n",
    "            </SIMPLE_METRICS>\n",
    "            <APPLY_TO_POSE>\n",
    "            </APPLY_TO_POSE>\n",
    "            <PROTOCOLS>\n",
    "                <Add mover_name=\"save_before_relax\" />\n",
    "                <Add mover_name=\"relax\"/>\n",
    "                <Add filter_name=\"buns\" />\n",
    "                <Add filter_name=\"sbuns\" />\n",
    "                <Add filter_name=\"vbuns\" />\n",
    "                <Add filter_name=\"cms\" />\n",
    "                <Add filter_name=\"exposed_hydrophobics\" />\n",
    "                <Add filter_name=\"geometry\"/>\n",
    "                <Add filter_name=\"mismatch_probability\" />\n",
    "                <Add filter_name=\"rmsd_final\" />\n",
    "                <Add metrics=\"sap_score\" />\n",
    "                <Add filter_name=\"score_per_res\" />\n",
    "                <Add filter_name=\"wnm_all\" />\n",
    "                <Add filter_name=\"wnm_hlx\" />\n",
    "            </PROTOCOLS>\n",
    "            <OUTPUT scorefxn=\"sfxn\" />\n",
    "        </ROSETTASCRIPTS>\n",
    "    \"\"\".format(\n",
    "        sfxn=sfxn,\n",
    "        pre=pre_break_helix,\n",
    "        post=pre_break_helix,\n",
    "    )\n",
    "    scored = SingleoutputRosettaScriptsTask(xml)\n",
    "    scored_ppose = scored(packed_pose_in.pose.clone())\n",
    "    pose = io.to_pose(scored_ppose)\n",
    "    pyrosetta.rosetta.core.pose.setPoseExtraScore(pose, \"parent\", parent)\n",
    "    scored_ppose = io.to_packed(pose)\n",
    "    return scored_ppose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup dask, set command line options, make tasks and submit to client for scoring ref\n",
    "Run `nstruct` of 5 to cover bases for stochasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from glob import glob\n",
    "import logging\n",
    "import pwd\n",
    "from pyrosetta.distributed.cluster.core import PyRosettaCluster\n",
    "\n",
    "\n",
    "print(\"run the following from your local terminal:\")\n",
    "print(\n",
    "    f\"ssh -L 8000:localhost:8787 {pwd.getpwuid(os.getuid()).pw_name}@{socket.gethostname()}\"\n",
    ")\n",
    "\n",
    "\n",
    "def create_tasks(selected, options):\n",
    "    for file in selected:\n",
    "        tasks = {\"options\": \"-corrections::beta_nov16 true\"}\n",
    "        tasks[\"extra_options\"] = options\n",
    "        tasks[\"-s\"] = file\n",
    "        if \"THR\" in file:\n",
    "            tasks[\"pre_break_helix\"] = 6\n",
    "        else:\n",
    "            tasks[\"pre_break_helix\"] = 4\n",
    "        yield tasks\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "pdbs = glob(os.path.join(os.getcwd(), \"00_inputs/*/*.pdb\"))\n",
    "\n",
    "options = {\n",
    "    \"-out:level\": \"300\",\n",
    "    \"-holes:dalphaball\": \"/home/bcov/ppi/tutorial_build/main/source/external/DAlpahBall/DAlphaBall.gcc\",\n",
    "    \"-indexed_structure_store:fragment_store\": \"/net/databases/VALL_clustered/connect_chains/ss_grouped_vall_helix_shortLoop.h5\",\n",
    "}\n",
    "\n",
    "output_path = os.path.join(os.getcwd(), \"05_score_ref\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # configure SLURM cluster as a context manager\n",
    "    with SLURMCluster(\n",
    "        cores=1,\n",
    "        processes=1,\n",
    "        job_cpu=1,\n",
    "        memory=\"4GB\",\n",
    "        queue=\"long\",\n",
    "        walltime=\"23:30:00\",\n",
    "        death_timeout=120,\n",
    "        local_directory=\"$TMPDIR/dask\",\n",
    "        log_directory=\"/mnt/home/pleung/logs/slurm_logs\",\n",
    "        extra=[\"--lifetime\", \"23h\", \"--lifetime-stagger\", \"4m\"],\n",
    "    ) as cluster:\n",
    "        print(cluster.job_script())\n",
    "        # scale between 1-1020 workers,\n",
    "        cluster.adapt(\n",
    "            minimum=1,\n",
    "            maximum=510,\n",
    "            wait_count=400,  # Number of consecutive times that a worker should be suggested for removal it is removed\n",
    "            interval=\"5s\",  # Time between checks\n",
    "        )\n",
    "        # setup a client to interact with the cluster as a context manager\n",
    "        with Client(cluster) as client:\n",
    "            print(client)\n",
    "            PyRosettaCluster(\n",
    "                tasks=create_tasks(pdbs, options),\n",
    "                client=client,\n",
    "                scratch_dir=output_path,\n",
    "                output_path=output_path,\n",
    "                nstruct=5,\n",
    "            ).distribute(protocols=[score_ref])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at scores\n",
    "Hacky function to load JSON-like data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_scorefile(scores):\n",
    "    import pandas as pd\n",
    "\n",
    "    scores = pd.read_json(scores, orient=\"records\", typ=\"frame\", lines=True)\n",
    "    scores = scores.T\n",
    "    mat = scores.values\n",
    "    n = mat.shape[0]\n",
    "    dicts = list(mat[range(n), range(n)])\n",
    "    index = scores.index\n",
    "    tabulated_scores = pd.DataFrame(dicts, index=index)\n",
    "    return tabulated_scores\n",
    "\n",
    "\n",
    "output_path = os.path.join(os.getcwd(), \"05_score_ref\")\n",
    "scores = os.path.join(output_path, \"scores.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get reference scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df = read_scorefile(scores)\n",
    "ref_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now need to retrieve the JSONs containing scores for the 4 msd runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "output_paths = glob(os.path.join(os.getcwd(), \"05_msd_runs_*/*/*.json\"))\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for test in tqdm(output_paths):\n",
    "    pdb = test.replace(\"json\", \"pdb\")\n",
    "    key = f\"{pdb}\"\n",
    "    with open(test, \"r\") as f:\n",
    "        values = json.load(f)\n",
    "    scores[key] = values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write them back to disk as a single scorefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df = scores_df.T\n",
    "output_path = os.path.join(os.getcwd(), \"05_filter\")\n",
    "output_file = os.path.join(output_path, \"scores.json\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "scores_df.to_json(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(os.getcwd(), \"05_filter\")\n",
    "scores = os.path.join(output_path, \"scores.json\")\n",
    "scores_df = pd.read_json(scores)\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which scores covary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    context=\"talk\",\n",
    "    font_scale=1,  # make the font larger; default is pretty small\n",
    "    style=\"ticks\",  # make the background white with black lines\n",
    "    palette=\"colorblind\",  # a color palette that is colorblind friendly!\n",
    ")\n",
    "\n",
    "sap_subset = scores_df[\n",
    "    [\n",
    "        \"sap_score_X\",\n",
    "        \"sap_score_Y\",\n",
    "        \"parent\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "ax = sns.pairplot(data=sap_subset.sample(frac=0.1), hue=\"parent\", corner=True, height=8)\n",
    "plt.suptitle(\"Correlation of SAP, split by parent\")\n",
    "sns.despine()\n",
    "plt.savefig(\"figs/05_correlations_sap_split_by_parent.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rho(x, y, ax=None, **kwargs):\n",
    "    \"\"\"Plot the correlation coefficient in the top left hand corner of a plot.\n",
    "    https://stackoverflow.com/questions/50832204/show-correlation-values-in-pairplot-using-seaborn-in-python/50835066\n",
    "    \"\"\"\n",
    "    import scipy\n",
    "\n",
    "    r, _ = scipy.stats.pearsonr(x, y)\n",
    "    ax = ax or plt.gca()\n",
    "    # Unicode for lowercase rho ()\n",
    "    rho = \"\\u03C1\"\n",
    "    ax.annotate(f\"{rho} = {r:.2f}\", xy=(0.1, 0.9), xycoords=ax.transAxes)\n",
    "\n",
    "\n",
    "ax = sns.pairplot(data=sap_subset.sample(frac=0.1), corner=True, height=8)\n",
    "ax.map_lower(rho)\n",
    "plt.suptitle(\"Correlation of SAP, with pearson R\")\n",
    "sns.despine()\n",
    "plt.savefig(\"figs/05_correlations_SAP_pearson.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sap_subset = scores_df[\n",
    "    [\n",
    "        \"exposed_hydrophobics_X\",\n",
    "        \"exposed_hydrophobics_Y\",\n",
    "        \"sap_score_X\",\n",
    "        \"sap_score_Y\",\n",
    "        \"parent\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "ax = sns.pairplot(data=sap_subset.sample(frac=0.1), hue=\"parent\", corner=True, height=8)\n",
    "plt.suptitle(\"Correlation of hydrophobicity, split by parent\")\n",
    "sns.despine()\n",
    "plt.savefig(\"figs/05_correlations_hydrophobicity_split_by_parent.png\")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "ax = sns.pairplot(data=sap_subset.sample(frac=0.1), corner=True, height=8)\n",
    "ax.map_lower(rho)\n",
    "plt.suptitle(\"Correlation of hydrophobicity, with pearson R\")\n",
    "sns.despine()\n",
    "plt.savefig(\"figs/05_correlations_hydrophobicity_pearson.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frag_subset = scores_df[\n",
    "    [\n",
    "        \"geometry_X\",\n",
    "        \"geometry_Y\",\n",
    "        \"mismatch_probability_X\",\n",
    "        \"mismatch_probability_Y\",\n",
    "        \"wnm_all_X\",\n",
    "        \"wnm_all_Y\",\n",
    "        \"wnm_hlx_X\",\n",
    "        \"wnm_hlx_Y\",\n",
    "        \"parent\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "ax = sns.pairplot(\n",
    "    data=frag_subset.sample(frac=0.1), hue=\"parent\", corner=True, height=8\n",
    ")\n",
    "plt.suptitle(\"Correlation of fragment metrics, split by parent\")\n",
    "sns.despine()\n",
    "plt.savefig(\"figs/05_correlations_frag_split_by_parent.png\")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "ax = sns.pairplot(data=frag_subset.sample(frac=0.1), corner=True, height=8)\n",
    "ax.map_lower(rho)\n",
    "plt.suptitle(\"Correlation of fragment metrics, with pearson R\")\n",
    "sns.despine()\n",
    "plt.savefig(\"figs/05_correlations_frag_pearson.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_unity(xdata, ydata, **kwargs):\n",
    "    \"\"\"https://stackoverflow.com/questions/48122019/how-can-i-plot-identity-lines-on-a-seaborn-pairplot\"\"\"\n",
    "    xmin, ymin = (xdata.min(), ydata.min())\n",
    "    xmax, ymax = (xdata.max(), ydata.max())\n",
    "    xpoints = np.linspace(xmin, xmax, 100)\n",
    "    ypoints = np.linspace(ymin, ymax, 100)\n",
    "    plt.gca().plot(\n",
    "        xpoints, ypoints, color=\"k\", marker=None, linestyle=\"--\", linewidth=1.0\n",
    "    )\n",
    "\n",
    "\n",
    "pack_subset = scores_df[\n",
    "    [\n",
    "        \"cms_X\",\n",
    "        \"cms_Y\",\n",
    "        \"score_per_res_X\",\n",
    "        \"score_per_res_Y\",\n",
    "        \"dslf_fa13_X\",\n",
    "        \"dslf_fa13_Y\",\n",
    "        \"sap_score_X\",\n",
    "        \"sap_score_Y\",\n",
    "        \"parent\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "ax = sns.pairplot(\n",
    "    data=pack_subset.sample(frac=0.1), hue=\"parent\", corner=True, height=8\n",
    ")\n",
    "plt.suptitle(\"Correlation of packing and score metrics, split by parent\")\n",
    "sns.despine()\n",
    "plt.savefig(\"figs/05_correlations_pack_split_by_parent.png\")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "ax = sns.pairplot(data=pack_subset.sample(frac=0.1), corner=True, height=8)\n",
    "ax.map_lower(rho)\n",
    "ax.map_offdiag(plot_unity)\n",
    "plt.suptitle(\"Correlation of packing and score metrics, with pearson R\")\n",
    "sns.despine()\n",
    "plt.savefig(\"figs/05_correlations_pack_pearson.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfortunately forgot to record `np_penalty` and `ala_penalty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for_plotting = scores_df[\n",
    "    [\n",
    "        \"buns_X\",\n",
    "        \"buns_Y\",\n",
    "        \"cms_X\",\n",
    "        \"cms_Y\",\n",
    "        \"exposed_hydrophobics_X\",\n",
    "        \"exposed_hydrophobics_Y\",\n",
    "        #         \"np_penalty\",\n",
    "        \"sap_score_X\",\n",
    "        \"sap_score_Y\",\n",
    "        \"sbuns_X\",\n",
    "        \"sbuns_Y\",\n",
    "        \"score_per_res_X\",\n",
    "        \"score_per_res_Y\",\n",
    "        \"vbuns_X\",\n",
    "        \"vbuns_Y\",\n",
    "        \"parent\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# for_plotting[\"np_penalty\"] = for_plotting[\"np_penalty\"].astype(int).astype(str)\n",
    "for term in [\n",
    "    \"buns\",\n",
    "    \"sbuns\",\n",
    "    \"vbuns\",\n",
    "    \"cms\",\n",
    "    \"exposed_hydrophobics\",\n",
    "    \"sap_score\",\n",
    "    \"score_per_res\",\n",
    "]:\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(30, 10), tight_layout=True)\n",
    "    sns.histplot(\n",
    "        ax=ax1, data=scores_df, x=f\"{term}_X\", y=f\"{term}_Y\", palette=\"colorblind\"\n",
    "    )\n",
    "    sns.histplot(ax=ax2, data=scores_df, x=f\"{term}_X\", kde=True, palette=\"colorblind\")\n",
    "    sns.histplot(ax=ax3, data=scores_df, x=f\"{term}_Y\", kde=True, palette=\"colorblind\")\n",
    "    plt.suptitle(f\"Pairwise analysis of {term}\")\n",
    "    sns.despine()\n",
    "    plt.savefig(f\"figs/05_pairwise_{term}.png\")\n",
    "    plt.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the good decoys for each state after removing really bad stuff\n",
    "`sap_score < 25`  \n",
    "`score_per_res < -3`  \n",
    "`vbuns < 3`  \n",
    "`wnm_all < 0.8`  \n",
    "`wnm_hlx < 0.15`  \n",
    "Won't filter on TRP or AAA just yet, if I did it would be:  \n",
    "`\"AAA\" < 3`  \n",
    "`\"W\" == 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def row2state(row):\n",
    "    state = (\n",
    "        row[\"parent\"]\n",
    "        + \"_p_\"\n",
    "        + str(int(row[\"pivot_helix\"]))\n",
    "        + \"_s_\"\n",
    "        + str(int(row[\"shift\"]))\n",
    "    )\n",
    "    return state\n",
    "\n",
    "\n",
    "strict_df = scores_df[scores_df[\"sap_score_X\"] < 25]\n",
    "strict_df = strict_df[strict_df[\"sap_score_Y\"] < 25]\n",
    "strict_df = strict_df[strict_df[\"score_per_res_X\"] < -3]\n",
    "strict_df = strict_df[strict_df[\"score_per_res_Y\"] < -3]\n",
    "strict_df = strict_df[strict_df[\"vbuns_X\"] < 1]\n",
    "strict_df = strict_df[strict_df[\"vbuns_Y\"] < 1]\n",
    "strict_df = strict_df[strict_df[\"wnm_all_X\"] < 0.8]\n",
    "strict_df = strict_df[strict_df[\"wnm_all_Y\"] < 0.8]\n",
    "strict_df = strict_df[strict_df[\"wnm_hlx_X\"] < 0.15]\n",
    "strict_df = strict_df[strict_df[\"wnm_hlx_Y\"] < 0.15]\n",
    "\n",
    "\n",
    "strict_df[\"state\"] = strict_df.apply(row2state, axis=1)\n",
    "\n",
    "print(len(set(strict_df.state.values)))\n",
    "print(len(strict_df))\n",
    "print(len(set(strict_df.parent.values)))\n",
    "set(scores_df.parent.values) - set(strict_df.parent.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump a list of pairs that pass strict cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), \"05_filter\", \"good.list\"), \"w\") as f:\n",
    "    for i in strict_df.index:\n",
    "        print(i, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup stuff to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = glob(os.path.join(os.getcwd(), \"05_msd_runs_*/*/*.json\"))\n",
    "\n",
    "for file in tqdm(to_delete):\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add average parent values to scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\n",
    "    \"buns\",\n",
    "    \"cms\",\n",
    "    \"mismatch_probability\",\n",
    "    \"sap_score\",\n",
    "    \"sbuns\",\n",
    "    \"score_per_res\",\n",
    "    \"wnm_all\",\n",
    "    \"wnm_hlx\",\n",
    "    \"vbuns\",\n",
    "]\n",
    "\n",
    "ref_means = {\n",
    "    \"buns\": dict(ref_df.groupby(\"parent\")[\"buns\"].mean()),\n",
    "    \"cms\": dict(ref_df.groupby(\"parent\")[\"cms\"].mean()),\n",
    "    \"sbuns\": dict(ref_df.groupby(\"parent\")[\"sbuns\"].mean()),\n",
    "    \"vbuns\": dict(ref_df.groupby(\"parent\")[\"vbuns\"].mean()),\n",
    "    \"mismatch_probability\": dict(\n",
    "        ref_df.groupby(\"parent\")[\"mismatch_probability\"].mean()\n",
    "    ),\n",
    "    \"sap_score\": dict(ref_df.groupby(\"parent\")[\"sap_score\"].mean()),\n",
    "    \"score_per_res\": dict(ref_df.groupby(\"parent\")[\"score_per_res\"].mean()),\n",
    "    \"wnm_all\": dict(ref_df.groupby(\"parent\")[\"wnm_all\"].mean()),\n",
    "    \"wnm_hlx\": dict(ref_df.groupby(\"parent\")[\"wnm_hlx\"].mean()),\n",
    "}\n",
    "\n",
    "\n",
    "def map_mean_parent_scores_to_row(row, term):\n",
    "    parent = row.parent\n",
    "    score = ref_means[term][parent]\n",
    "    return score\n",
    "\n",
    "\n",
    "for term in tqdm(terms):\n",
    "    scores_df[f\"{term}_parent\"] = scores_df.apply(\n",
    "        lambda row: map_mean_parent_scores_to_row(row, term),\n",
    "        axis=1,\n",
    "    )\n",
    "scores_df.to_json(\"05_filter/scores_combined_with_parents.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    context=\"talk\",\n",
    "    font_scale=2,  # make the font larger; default is pretty small\n",
    "    style=\"ticks\",  # make the background white with black lines\n",
    "    palette=\"colorblind\",  # a color palette that is colorblind friendly!\n",
    ")\n",
    "terms = [\n",
    "    \"buns\",\n",
    "    \"cms\",\n",
    "    \"mismatch_probability\",\n",
    "    \"sap_score\",\n",
    "    \"sbuns\",\n",
    "    \"score_per_res\",\n",
    "    \"wnm_all\",\n",
    "    \"wnm_hlx\",\n",
    "    \"vbuns\",\n",
    "]\n",
    "order = sorted(list(set(scores_df.parent.values)))\n",
    "for term in terms:\n",
    "    for state in [\"X\", \"Y\"]:\n",
    "        fig = plt.figure(figsize=(30, 20), tight_layout=True)\n",
    "        plt.xticks(rotation=90)\n",
    "        sns.boxplot(\n",
    "            x=\"parent\",\n",
    "            y=term + \"_\" + state,\n",
    "            data=scores_df.sample(frac=0.2),\n",
    "            showfliers=False,\n",
    "            order=order,\n",
    "        )\n",
    "        sns.stripplot(\n",
    "            x=\"parent\",\n",
    "            y=term + \"_\" + state,\n",
    "            data=scores_df.sample(frac=0.2),\n",
    "            order=order,\n",
    "        )\n",
    "        sns.stripplot(\n",
    "            x=\"parent\",\n",
    "            y=term + \"_parent\",\n",
    "            data=scores_df.sample(frac=0.2),\n",
    "            color=\"black\",\n",
    "            order=order,\n",
    "        )\n",
    "        sns.despine()\n",
    "        plt.title(term + \"_\" + state)\n",
    "        plt.show()\n",
    "        fig.savefig(\n",
    "            \"figs/05_before_selection_all_parents_vs_{term}_{state}.png\".format(\n",
    "                term=term, state=state\n",
    "            )\n",
    "        )\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocks I didn't use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyrosetta.distributed import cluster\n",
    "\n",
    "pyrosetta.distributed.init(\n",
    "    \"-out:level 500 -corrections::beta_nov16 true -holes:dalphaball /home/bcov/ppi/tutorial_build/main/source/external/DAlpahBall/DAlphaBall.gcc -indexed_structure_store:fragment_store /net/databases/VALL_clustered/connect_chains/ss_grouped_vall_helix_shortLoop.h5 -dunbrack_prob_buried 0.5 -dunbrack_prob_nonburied 0.5 -dunbrack_prob_buried_semi 0.5 -dunbrack_prob_nonburied_semi 0.5\"\n",
    ")\n",
    "kwargs_ = {\n",
    "    \"-s\": \"/mnt/home/pleung/projects/bistable_bundle/r4/helix_binders/04_pairs/decoys/0000/2021.06.16.09.05.57.348631_6b13ba8b8f04450a885e127d5e66bcdf.pdb.bz2\",\n",
    "    \"-x\": \"/mnt/home/pleung/projects/bistable_bundle/r4/helix_binders/04_staple_ref/decoys/0000/2021.06.16.00.40.13.771912_aec12a83734b4f66a196eace6dcd0910.pdb.bz2\",\n",
    "    \"ala_pen\": 0,\n",
    "    \"np_pen\": 0.01,\n",
    "}\n",
    "\n",
    "tpose = msd(None, **kwargs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sequences with hacky `.sh` script\n",
    "Append all sequences to a FASTA:  \n",
    "`while read p; do ./pdb2fasta.sh \"$p\" ; done < 05_filter/good.list >> temp.fasta`  \n",
    "Get only the chain A sequences (chain A and chain B have the same sequence) and flatten the FASTA:  \n",
    "`cat temp.fasta | sed ':a;N;$!ba;s/chain A\\n/chainA /g' | grep chainA | sed 's/ chainA / /' > temp2.fasta; rm temp.fasta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!while read p; do ./pdb2fasta.sh \"$p\" ; done < 05_filter/good.list >> temp.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat temp.fasta | sed ':a;N;$!ba;s/chain A\\n/chainA /g' | grep chainA | sed 's/ chainA / /' > temp2.fasta; rm temp.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make mapping of fasta headers to df keys\n",
    "hash2key = {}\n",
    "\n",
    "for key in strict_df.index:\n",
    "    hash_ = key.split(\"/\")[-1].replace(\".pdb\", \"\")\n",
    "    hash2key[hash_] = key\n",
    "\n",
    "# Add in sequence info to df\n",
    "strict_df[\"sequence\"] = \"\"\n",
    "\n",
    "with open(\"temp2.fasta\", \"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        hash_, seq = line.split()\n",
    "        key = hash2key[hash_.replace(\">\", \"\")]\n",
    "        strict_df.loc[key, \"sequence\"] = seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove sequences containing != 1 TRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idces = []\n",
    "for i, row in strict_df.iterrows():\n",
    "    if \"W\" in row[\"sequence\"]:\n",
    "        if row[\"sequence\"].count(\"AAA\") < 2:\n",
    "            if row[\"sequence\"].count(\"W\") == 1:\n",
    "                idces.append(i)\n",
    "\n",
    "one_trp = strict_df.loc[idces]\n",
    "print(len(set(one_trp.state.values)))\n",
    "print(len(one_trp))\n",
    "set(scores_df.parent.values) - set(one_trp.parent.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump a list of pairs that pass strict cutoffs and have 1 TRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), \"05_filter\", \"1trp_good.list\"), \"w\") as f:\n",
    "    for i in one_trp.index:\n",
    "        print(i, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_trp.to_json(os.path.join(os.getcwd(), \"05_filter\", \"1trp_good.json\"))\n",
    "!rm temp2.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_trp = pd.read_json(os.path.join(os.getcwd(), \"05_filter\", \"1trp_good.json\"))\n",
    "for term in tqdm(terms):\n",
    "    one_trp[f\"{term}_parent\"] = one_trp.apply(\n",
    "        lambda row: map_mean_parent_scores_to_row(row, term),\n",
    "        axis=1,\n",
    "    )\n",
    "order = sorted(list(set(one_trp.parent.values)))\n",
    "for term in terms:\n",
    "    for state in [\"X\", \"Y\"]:\n",
    "        fig = plt.figure(figsize=(30, 20), tight_layout=True)\n",
    "        plt.xticks(rotation=90)\n",
    "        sns.boxplot(\n",
    "            x=\"parent\",\n",
    "            y=term + \"_\" + state,\n",
    "            data=one_trp,\n",
    "            showfliers=False,\n",
    "            order=order,\n",
    "        )\n",
    "        sns.stripplot(\n",
    "            x=\"parent\",\n",
    "            y=term + \"_\" + state,\n",
    "            data=one_trp,\n",
    "            order=order,\n",
    "        )\n",
    "        sns.stripplot(\n",
    "            x=\"parent\",\n",
    "            y=term + \"_parent\",\n",
    "            data=one_trp,\n",
    "            color=\"black\",\n",
    "            order=order,\n",
    "        )\n",
    "        sns.despine()\n",
    "        plt.title(term + \"_\" + state)\n",
    "        plt.show()\n",
    "        fig.savefig(\n",
    "            \"figs/05_after_selection_all_parents_vs_{term}_{state}.png\".format(\n",
    "                term=term, state=state\n",
    "            )\n",
    "        )\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurize the designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from glob import glob\n",
    "import logging\n",
    "import pwd\n",
    "\n",
    "\n",
    "def featurize(abspath):\n",
    "    import os, subprocess\n",
    "\n",
    "    def cmd(command, wait=True):\n",
    "        \"\"\"@nrbennet @bcov\"\"\"\n",
    "        the_command = subprocess.Popen(\n",
    "            command,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            universal_newlines=True,\n",
    "        )\n",
    "        if not wait:\n",
    "            return\n",
    "        the_stuff = the_command.communicate()\n",
    "        return str(the_stuff[0]) + str(the_stuff[1])\n",
    "\n",
    "    working_dir = os.path.dirname(abspath)\n",
    "    pythonpath = \"/software/conda/envs/tensorflow/bin/python\"\n",
    "    script = os.path.join(os.getcwd(), \"predict_.py\")\n",
    "    pdb2fasta = os.path.join(os.getcwd(), \"pdb2fasta.sh\")\n",
    "    pdb = os.path.basename(abspath)\n",
    "    handle = pdb.replace(\"pdb\", \"fasta\")\n",
    "    to_send = f\"\"\"cd {working_dir}; {pdb2fasta} {pdb} > {handle} ;\"\"\"\n",
    "    to_send += f\"\"\" {pythonpath} {script} -i {handle} -o \"/net/scratch/pleung/\" \"\"\"\n",
    "    print(\"sending: \", to_send)\n",
    "    cmd(to_send)\n",
    "    return\n",
    "\n",
    "\n",
    "print(\"run the following from your local terminal:\")\n",
    "print(\n",
    "    f\"ssh -L 8000:localhost:8787 {pwd.getpwuid(os.getuid()).pw_name}@{socket.gethostname()}\"\n",
    ")\n",
    "\n",
    "futures = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # configure SLURM cluster as a context manager\n",
    "    with SLURMCluster(\n",
    "        cores=1,\n",
    "        processes=1,\n",
    "        job_cpu=1,\n",
    "        memory=\"4GB\",\n",
    "        queue=\"long\",\n",
    "        walltime=\"23:30:00\",\n",
    "        death_timeout=120,\n",
    "        local_directory=\"$TMPDIR/dask\",\n",
    "        log_directory=\"/mnt/home/pleung/logs/slurm_logs\",\n",
    "        extra=[\"--lifetime\", \"23h\", \"--lifetime-stagger\", \"4m\"],\n",
    "    ) as cluster:\n",
    "        print(cluster.job_script())\n",
    "        # scale between 1-510 workers,\n",
    "        cluster.adapt(\n",
    "            minimum=1,\n",
    "            maximum=510,\n",
    "            wait_count=400,  # Number of consecutive times that a worker should be suggested for removal it is removed\n",
    "            interval=\"5s\",  # Time between checks\n",
    "        )\n",
    "        # setup a client to interact with the cluster as a context manager\n",
    "        with Client(cluster) as client:\n",
    "            print(client)\n",
    "            with open(\n",
    "                os.path.join(os.getcwd(), \"05_filter\", \"1trp_good.list\"), \"r\"\n",
    "            ) as f:\n",
    "                for pdb in f:\n",
    "                    future = client.submit(featurize, pdb.rstrip())\n",
    "                    futures.append(future)\n",
    "            results = [pending.result() for pending in futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = (\n",
    "    glob(os.path.join(os.getcwd(), \"05_msd_runs_*/*/*.json\"))\n",
    "    + glob(os.path.join(os.getcwd(), \"05_msd_runs_*/*/*.scwrl4.pdb\"))\n",
    "    + glob(os.path.join(os.getcwd(), \"05_msd_runs_*/*/*_0.pdb\"))\n",
    "    + glob(os.path.join(os.getcwd(), \"05_msd_runs_*/*/*_1.pdb\"))\n",
    "    + glob(os.path.join(os.getcwd(), \"05_msd_runs_*/*/*.fasta\"))\n",
    ")\n",
    "\n",
    "for file in tqdm(to_delete):\n",
    "    os.remove(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phil (3.8.2)",
   "language": "python",
   "name": "phil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
